{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing IT Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Sources\n",
    "\n",
    "Provided by the Application:\n",
    "* Logs\n",
    "* Status endpoints (`/stats.json`, `/proc`)\n",
    "* Instrumentation (`StatsD.gauge('Base.queued', 12)`, Zipkin)\n",
    "\n",
    "Provided by the Runtime:\n",
    "* JMX, PyMX\n",
    "* Debuggers (gdb, jdb, pdb)\n",
    "* Profilers\n",
    "\n",
    "Provided by the Kernel:\n",
    "* Dynamic tracing (DTrace, eBPF) \n",
    "* System profilers (perf)\n",
    "* Network sniffing (pcap)\n",
    "\n",
    "Provided by Hardware:\n",
    "* PCM/MSR registers (Intel)\n",
    "* SMART disk interface\n",
    "* RAID card interfacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common Denominator: Events\n",
    "\n",
    "All the above data sources can be regarded as emitting events:\n",
    "\n",
    "```\n",
    "{ event=\"HTTP_START\", reqid=2398721, endpoint=\"/\", srcip=\"10.8.20.132\", t=1535120313.123 }\n",
    "{ event=\"HTTP_END\", reqid=2398721, status=\"OK\", bytes=125232, t=1535120313.123  }\n",
    "{ event=\"HTTP\", endpoint=\"/\", duration[ms]=1.23, status=\"200\", srcip=\"10.8.20.132\", t=1535120313.123 }\n",
    "{ event=\"funccall\", name=\"Customer.payment\", args=[120, \"USD\"], t=1535120313.123 }\n",
    "{ event=\"cpu_idle\", value=1231522, unit=jiffy, t=1535120313 }\n",
    "{ event=\"syscall\", name=\"read\", pid=1231, duration[us]=18.1, t=1535120313.123123152 }\n",
    "{ event=\"shedule_on_cpu\", pid=1231 , t=1535120313.123123152 }\n",
    "{ event=\"shedule_off_cpu\", pid=1231, t=1535120313.123123152 }\n",
    "```\n",
    "\n",
    "<!--\n",
    "\n",
    "Remarks:\n",
    "- Events seem to be used as a term for state changes.\n",
    "- Some of the data sources are not attached to state changes, e.g. \"disk free %\", but more to the state the system is in. We can introduce measurement \"events\", to make that fit our description here.\n",
    "- Events as K-V pairs is relatively arbitrary.\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Geometric View on Events\n",
    "\n",
    "Events are points in a high dimensional space:\n",
    "\n",
    "- Each attribute name gives an axes.\n",
    "- Each attribute value, determines the location on that axes.\n",
    "- Attribues that are not set are set to a special value `undef`.\n",
    "\n",
    "![](../img/events.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Challenges\n",
    "\n",
    "- High Dimensionality: Lots of axes\n",
    "- High Cardinality: Lots of values on a single discrete axes (userid, pid)\n",
    "- High Volume: Lots of events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Coping Strategies\n",
    "\n",
    "* Forget attributes (reduce dimensionality)  \n",
    "  E.g. only forget \"srcip\", \"userid\" attibutes\n",
    "\n",
    "* Filter by attribute values (reduce dimensionality, volume)  \n",
    "  E.g. only record events with user_id=25\n",
    "\n",
    "* Group values (reduce cardinality)  \n",
    "  E.g. ip => ip range (192.*) or code=204 => 2xx\n",
    "\n",
    "* **Sampling** (reduce volume)  \n",
    "  E.g. only record every 100s HTTP request.\n",
    "\n",
    "* **Aggregation** (reduce volume)  \n",
    "  over time (within this minute) and across dimensions (e.g. users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Models\n",
    "\n",
    "(1) Documents\n",
    "- Events are naturally expressed as JSON documents\n",
    "\n",
    "- Used by Log Analysis and APM tools: ELK (ElasticSearch/Lucene), Splunk, Honeycomb, Dynatrace ([Cassandra](https://www.dynatrace.com/platform/dynatrace-architecture/))\n",
    "\n",
    "(2) Tables\n",
    "- Pre-selection of attributes allows to impose table structure\n",
    "\n",
    "- Used by APM tools: NewRelic ([MySQL](http://highscalability.com/blog/2011/7/18/new-relic-architecture-collecting-20-billion-metrics-a-day.html)), VividCortext ([MySQL](http://highscalability.com/blog/2015/3/30/how-we-scale-vividcortexs-backend-systems.html))\n",
    "\n",
    "(3) Time Series\n",
    "- Record a single attribute changing over time\n",
    "- Works like a three column table `(KEY, TIME, VALUE)`\n",
    "- Discrete TIME axes (10s, 60s) typical\n",
    "- Continues VALUE axes (float) typical\n",
    "- More and more structure is allowed in the KEY field (uuid ~> tag set)\n",
    "\n",
    "- Used by Monitoring systems, APM tools: Graphite, Prometheues, Influx, Circonus/IRONdb, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tradeoff\n",
    "\n",
    "* Metrics are compact, aggregated, aggregatable, efficient, but canâ€™t be disaggregated.\n",
    "\n",
    "* Logs/events are full fidelity but relatively expensive at full capture.\n",
    "\n",
    "Source: https://www.xaprb.com/slides/devopsdc-what-is-observability/#9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log volume per day: 259.2 MB\n"
     ]
    }
   ],
   "source": [
    "# Numeric Example: Storage Volume Logs vs. Metrics\n",
    "\n",
    "# Access logs\n",
    "req_per_second = 3\n",
    "log_lines_per_req = 1\n",
    "bytes_per_log_line = 1000\n",
    "seconds_per_day = 24*60*60\n",
    "bytes_per_kb = 1000\n",
    "bytes_per_mb = 1000 * bytes_per_kb\n",
    "log_mb_per_day = bytes_per_log_line * log_lines_per_req * req_per_second * seconds_per_day / bytes_per_mb\n",
    "print(\"Log volume per day: {:,.1f} MB\".format(log_mb_per_day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric volume per day: 0.115 MB\n"
     ]
    }
   ],
   "source": [
    "# Metric Storage: request-rate, error-rate, p50, p95, p99\n",
    "number_of_metrics = 5\n",
    "bytes_per_value = 8 + 8 # value + timestamp\n",
    "aggregation_period_sec = 60\n",
    "values_per_day = number_of_metrics * seconds_per_day / aggregation_period_sec\n",
    "metric_bytes_per_day = bytes_per_value * values_per_day\n",
    "print(\"Metric volume per day: {:.3f} MB\".format(metric_bytes_per_day/bytes_per_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logs\n",
    "\n",
    "* Capture full information about an event that occured including all attributes\n",
    "* Long-lines should be machine readable\n",
    "* Various levles of indexing seen in practice\n",
    "* Very expensive to store in the medium term\n",
    "\n",
    "Coping strategies:\n",
    "* Denormalize events\n",
    "* Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Metrics\n",
    "We say that:\n",
    "\n",
    "> \"Events are rolled-up into metrics.\"\n",
    "\n",
    "It's a three step process:\n",
    "\n",
    "* (1) Select Axes\n",
    "* (2) Group by Time\n",
    "* (3) Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (1) Select Axes\n",
    "\n",
    "A metric has a single numeric value axes (e.g. \"duration\").\n",
    "In addition it can take arbitraty attributes as a metric key.\n",
    "\n",
    "For each event attribute we have the choice to:\n",
    "\n",
    "* Filter by attribute value (e.g. event=HTTP)\n",
    "\n",
    "* Keep attribute and make it part of the metric key (e.g. `{host=www1,dc=ir/dub}`)\n",
    "\n",
    "* Forget attribute (e.g. ignore user_id)\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```\n",
    "SELECT t, (host, dc) as key, duration as value FROM events WHERE event='HTTP';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution:** Keeping High Cardinality attributes let's the metric count explode (e.g. user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Group by Time\n",
    "\n",
    "If you measure time precise enough all events will have different time stamps.\n",
    "\n",
    "Need to group time values into discrete windows. Typical \"periods\" 10s, 60s, 5M.\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```\n",
    "SELECT (floor(t/period) as T, duration) FROM events WHERE event='HTTP', host='www1-eu-dus';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### (3) Aggregate Events\n",
    "\n",
    "All durations in the given time window, need to be summarized to an aggregate.\n",
    "Typical aggregation functions are:\n",
    "\n",
    "* count\n",
    "* mean\n",
    "* sum\n",
    "* min/max\n",
    "* percentile\n",
    "* histogram\n",
    "\n",
    "Equivalent SQL:\n",
    "\n",
    "```\n",
    "SELECT \n",
    "   floor(t/period) as T, \n",
    "   aggregate(duration)\n",
    "FROM events \n",
    "WHERE \n",
    "  event='HTTP', host='www1-eu-dus' \n",
    "GROUP BY 1; # first field\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary: Creating Metrics from Events\n",
    "\n",
    "<img src=\"../img/metrics.png\" style=\"width:800px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Higher Rollups\n",
    "\n",
    "So far we were only concerned with rolling-up metrics to a base period of 60s.\n",
    "For graphing, indexing, and long-term storage also rollups on hiher periods are used:\n",
    "\n",
    "<img src=\"../img/rollup.png\" style=\"height:600px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Properties for Rollup Aggregation\n",
    "\n",
    "## Robustness \n",
    "\n",
    "Data is noisy. We don't want a single outlier to skew up the aggregate.\n",
    "\n",
    "An aggregation method is *robust* if a small number of (exteme) outliers do affect the aggregate only a little.\n",
    "\n",
    "## Mergability\n",
    "\n",
    "Mergability the property of an aggregation method, that let's you aggregate aggregates.\n",
    "\n",
    "* Critical for computing higher rollups, and graphing.\n",
    "* Critical for cross metric aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics for Engineers\n",
    "\n",
    "What are we doing here today?\n",
    "\n",
    "* Visualizing Events and Metrics\n",
    "* Aggregating Events and Metrics\n",
    "* Sampling Events\n",
    "* Modeling and prediction of systems behavior"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
